{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "african-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import pickle5\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pysurvival.models.semi_parametric import CoxPHModel\n",
    "from pysurvival.utils.display import display_loss_values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pysurvival.utils.display import compare_to_actual\n",
    "from pysurvival.utils import save_model, load_model\n",
    "from pysurvival.utils.sklearn_adapter import sklearn_adapter\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import auc, r2_score\n",
    "from sklearn.utils import resample\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn import metrics\n",
    "## block warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "prescribed-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>life_time</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_end</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance_center_km</th>\n",
       "      <th>weekday_b</th>\n",
       "      <th>charging_ports</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag2</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_b_22</th>\n",
       "      <th>hour_b_23</th>\n",
       "      <th>Station_Name_BOULDER / N BOULDER REC 1</th>\n",
       "      <th>Station_Name_COMM VITALITY / 1000WALNUT</th>\n",
       "      <th>Station_Name_COMM VITALITY / 1104 SPRUCE1</th>\n",
       "      <th>4Hsplit</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>weekday_b_name</th>\n",
       "      <th>tod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>118.5</td>\n",
       "      <td>2018-01-05 14:25:00</td>\n",
       "      <td>2018-01-05 16:23:30</td>\n",
       "      <td>40.000148</td>\n",
       "      <td>-105.282437</td>\n",
       "      <td>2.096847</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1271.5</td>\n",
       "      <td>135.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BOULDER / BASELINE ST1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>886.5</td>\n",
       "      <td>2018-01-05 17:02:00</td>\n",
       "      <td>2018-01-06 07:48:30</td>\n",
       "      <td>40.000148</td>\n",
       "      <td>-105.282437</td>\n",
       "      <td>2.096847</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>118.5</td>\n",
       "      <td>1271.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>BOULDER / BASELINE ST1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>326.5</td>\n",
       "      <td>2018-01-06 09:28:00</td>\n",
       "      <td>2018-01-06 14:54:30</td>\n",
       "      <td>40.000148</td>\n",
       "      <td>-105.282437</td>\n",
       "      <td>2.096847</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>886.5</td>\n",
       "      <td>118.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>BOULDER / BASELINE ST1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      life_time      datetime_start        datetime_end   latitude  \\\n",
       "3734      118.5 2018-01-05 14:25:00 2018-01-05 16:23:30  40.000148   \n",
       "3735      886.5 2018-01-05 17:02:00 2018-01-06 07:48:30  40.000148   \n",
       "3736      326.5 2018-01-06 09:28:00 2018-01-06 14:54:30  40.000148   \n",
       "\n",
       "       longitude  distance_center_km  weekday_b  charging_ports    lag1  \\\n",
       "3734 -105.282437            2.096847          4               2  1271.5   \n",
       "3735 -105.282437            2.096847          4               2   118.5   \n",
       "3736 -105.282437            2.096847          5               2   886.5   \n",
       "\n",
       "        lag2  ...  hour_b_22  hour_b_23  \\\n",
       "3734   135.5  ...          0          0   \n",
       "3735  1271.5  ...          0          0   \n",
       "3736   118.5  ...          0          0   \n",
       "\n",
       "      Station_Name_BOULDER / N BOULDER REC 1  \\\n",
       "3734                                       0   \n",
       "3735                                       0   \n",
       "3736                                       0   \n",
       "\n",
       "      Station_Name_COMM VITALITY / 1000WALNUT  \\\n",
       "3734                                        0   \n",
       "3735                                        0   \n",
       "3736                                        0   \n",
       "\n",
       "      Station_Name_COMM VITALITY / 1104 SPRUCE1  4Hsplit  y_pred  \\\n",
       "3734                                          0        0       1   \n",
       "3735                                          0        1       1   \n",
       "3736                                          0        1       0   \n",
       "\n",
       "                Station_Name  weekday_b_name        tod  \n",
       "3734  BOULDER / BASELINE ST1          Friday     Midday  \n",
       "3735  BOULDER / BASELINE ST1          Friday  Afternoon  \n",
       "3736  BOULDER / BASELINE ST1        Saturday    Morning  \n",
       "\n",
       "[3 rows x 102 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/ut_V6_classification-4H.pkl\", \"rb\") as fh:\n",
    "    df = pickle5.load(fh)\n",
    "    \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-april",
   "metadata": {},
   "source": [
    "### Usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "competitive-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get point predictions\n",
    "def point_pred_single(model, X_test):\n",
    "    T_pred = []\n",
    "    # Get survival curves\n",
    "    cph_pred = model.predict_survival(X_test)\n",
    "    # get times of survival prediction\n",
    "    time = model.times\n",
    "    # test\n",
    "    for i in range(0,len(cph_pred)):\n",
    "        T_pred.append(auc(time,cph_pred[i]))\n",
    "    \n",
    "    return T_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "based-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get point predictions\n",
    "def point_pred(model, X_test, X_train):\n",
    "    T_pred = []\n",
    "    T_pred_train = []\n",
    "    # Get survival curves\n",
    "    cph_pred = model.predict_survival(X_test)\n",
    "    cph_pred_train = model.predict_survival(X_train)\n",
    "    # get times of survival prediction\n",
    "    time = model.times\n",
    "    # test\n",
    "    for i in range(0,len(cph_pred)):\n",
    "        T_pred.append(auc(time,cph_pred[i]))\n",
    "    # train\n",
    "    for i in range(0,len(cph_pred_train)):\n",
    "        T_pred_train.append(auc(time,cph_pred_train[i]))\n",
    "    \n",
    "    return T_pred, T_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "competitive-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deluxe-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To test model\n",
    "def test_model(y_test, y_pred):\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    #print('MAE (Mean Absolute Error):', MAE)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "    #print('MSE (Mean Squared Error):', MSE)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    #print('RMSE (Root Mean Squared Error):', RMSE)\n",
    "    MAPE = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    NRMSE = RMSE/np.mean(y_test)\n",
    "    return MAE, RMSE, MSE, MAPE, NRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-mills",
   "metadata": {},
   "source": [
    "## Modeling features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-thomas",
   "metadata": {},
   "source": [
    "To make coding easier the names of the models are changed:\n",
    "- M1: Baseline\n",
    "- M5: Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dressed-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column = 'life_time'\n",
    "event_column = 'event'\n",
    "\n",
    "## M1\n",
    "features1 = ['weekday_b_name_Monday', 'weekday_b_name_Saturday',\n",
    "             'weekday_b_name_Sunday', 'weekday_b_name_Thursday',\n",
    "             'weekday_b_name_Tuesday', 'weekday_b_name_Wednesday', 'tod_Evening', 'tod_Midday', 'tod_Morning',\n",
    "       'Station_Name_BOULDER / N BOULDER REC 1',\n",
    "       'Station_Name_COMM VITALITY / 1000WALNUT',\n",
    "       'Station_Name_COMM VITALITY / 1104 SPRUCE1'] # dow + tod\n",
    "\n",
    "## M5\n",
    "features5_cat = ['weekday_b_name_Monday', 'weekday_b_name_Saturday',\n",
    "             'weekday_b_name_Sunday', 'weekday_b_name_Thursday',\n",
    "             'weekday_b_name_Tuesday', 'weekday_b_name_Wednesday','tod_Evening', 'tod_Midday', 'tod_Morning',\n",
    "       'Station_Name_BOULDER / N BOULDER REC 1',\n",
    "       'Station_Name_COMM VITALITY / 1000WALNUT',\n",
    "       'Station_Name_COMM VITALITY / 1104 SPRUCE1']\n",
    "features5_con = ['lag1', 'lag2', 'lag3','near_charge_time_4H', 'near_charge_energy_4H',\n",
    "                 'charge_time_4H', 'charge_energy_4H'] # dow + agg. tod + lag + activ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "front-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data split\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "african-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event'] = np.ones(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "popular-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (4088, 103)\n",
      "Testing shape: (1022, 103)\n"
     ]
    }
   ],
   "source": [
    "train, test = np.split(df, [int(split * len(df))])\n",
    "print(\"Training shape:\",train.shape)\n",
    "print(\"Testing shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "psychological-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainS = train[train['y_pred'] == 0]\n",
    "trainL = train[train['y_pred'] == 1]\n",
    "\n",
    "testS = test[test['y_pred'] == 0]\n",
    "testL = test[test['y_pred'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "signal-transsexual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORT: Train: 2433 Test: 629\n",
      "LONG: Train: 1655 Test: 393\n"
     ]
    }
   ],
   "source": [
    "print(\"SHORT: Train: {} Test: {}\".format(trainS.shape[0], testS.shape[0]))\n",
    "print(\"LONG: Train: {} Test: {}\".format(trainL.shape[0], testL.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fatty-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get X \n",
    "# Baseline\n",
    "X1_S = trainS[features1]\n",
    "X1_L = trainL[features1]\n",
    "\n",
    "## Full\n",
    "# Short\n",
    "X5_S = trainS[features5_cat+features5_con]\n",
    "scaler = StandardScaler()\n",
    "X5_S[features5_con] = scaler.fit_transform(X5_S[features5_con])\n",
    "X5_S_test = testS[features5_cat+features5_con]\n",
    "X5_S_test[features5_con] = scaler.transform(X5_S_test[features5_con])\n",
    "# Full\n",
    "X5_L = trainL[features5_cat+features5_con]\n",
    "scaler = StandardScaler()\n",
    "X5_L[features5_con] = scaler.fit_transform(X5_L[features5_con])\n",
    "X5_L_test = testL[features5_cat+features5_con]\n",
    "X5_L_test[features5_con] = scaler.transform(X5_L_test[features5_con])\n",
    "\n",
    "## Get E -train\n",
    "E_S = trainS[event_column]\n",
    "E_L = trainL[event_column]\n",
    "\n",
    "## Get T - train\n",
    "T_S = trainS['life_time']\n",
    "T_L = trainL['life_time']\n",
    "\n",
    "## Get E -Test\n",
    "E_S_test = testS[event_column]\n",
    "E_L_test = testL[event_column]\n",
    "\n",
    "## Get T - Test\n",
    "T_S_test = testS['life_time']\n",
    "T_L_test = testL['life_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-friday",
   "metadata": {},
   "source": [
    "### Find hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "scenic-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "regularization = [0.01, 0.1, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-frequency",
   "metadata": {},
   "source": [
    "Short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "detected-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST!! 0\n",
      "0.11   0.22   0.33   NEW BEST!! 3\n",
      "0.44   NEW BEST!! 4\n",
      "0.56   0.67   The gradient exploded.You should reduce the learning rate (lr) or try a different initialization.\n",
      "FAIL, lr: 0.1  l2: 0.01\n",
      "0.78   The gradient exploded.You should reduce the learning rate (lr) or try a different initialization.\n",
      "FAIL, lr: 0.1  l2: 0.1\n",
      "0.89   1.0   Parameter tuning done.\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "### Find most optimal parameters for every station\n",
    "best_c = -1\n",
    "best_lear = None\n",
    "best_regu = None\n",
    "best_model = None\n",
    "fails = {}\n",
    "\n",
    "total_it = len(learning_rates)*len(regularization)\n",
    "i=0\n",
    "\n",
    "# loop over parameters\n",
    "for lear in learning_rates:\n",
    "    for regu in regularization:\n",
    "        try: \n",
    "            cph = CoxPHModel()\n",
    "            cph.fit(X1_S, T_S, E_S,\n",
    "                    lr=lear, l2_reg = regu, init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "            #point pred\n",
    "            point_predictions = point_pred_single(cph,X1_S)\n",
    "\n",
    "            # c-index\n",
    "            c_temp = concordance_index(T_S, point_predictions)\n",
    "\n",
    "            if c_temp >= best_c:\n",
    "                print(\"NEW BEST!!\", i)\n",
    "                best_lear = lear\n",
    "                best_regu = regu\n",
    "                best_c = c_temp  \n",
    "        except:\n",
    "            print(\"FAIL, lr:\", lear, \" l2:\", regu)   \n",
    "        i+=1\n",
    "        print(round(i/total_it,2), \"  \", end='')\n",
    "print(\"Parameter tuning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacterial-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG, M1: lr: 0.01 l2_reg: 0.1\n"
     ]
    }
   ],
   "source": [
    "lr_s1 = best_lear\n",
    "l2_reg_s1 = best_regu\n",
    "\n",
    "print(\"SHORT, M1: lr: {} l2_reg: {}\".format(best_lear,best_regu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solved-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST!! 0\n",
      "0.11   NEW BEST!! 1\n",
      "0.22   NEW BEST!! 2\n",
      "0.33   NEW BEST!! 3\n",
      "0.44   0.56   0.67   The gradient exploded.You should reduce the learning rate (lr) or try a different initialization.\n",
      "FAIL, lr: 0.1  l2: 0.01\n",
      "0.78   The gradient exploded.You should reduce the learning rate (lr) or try a different initialization.\n",
      "FAIL, lr: 0.1  l2: 0.1\n",
      "0.89   1.0   Parameter tuning done.\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "### Find most optimal parameters for every station\n",
    "best_c = -1\n",
    "best_lear = None\n",
    "best_regu = None\n",
    "best_model = None\n",
    "fails = {}\n",
    "\n",
    "total_it = len(learning_rates)*len(regularization)\n",
    "i=0\n",
    "\n",
    "# loop over parameters\n",
    "for lear in learning_rates:\n",
    "    for regu in regularization:\n",
    "        try: \n",
    "            cph = CoxPHModel()\n",
    "            cph.fit(X5_S, T_S, E_S,\n",
    "                    lr=lear, l2_reg = regu, init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "            #point pred\n",
    "            point_predictions = point_pred_single(cph,X5_S)\n",
    "\n",
    "            # c-index\n",
    "            c_temp = concordance_index(T_S, point_predictions)\n",
    "\n",
    "            if c_temp >= best_c:\n",
    "                print(\"NEW BEST!!\", i)\n",
    "                best_lear = lear\n",
    "                best_regu = regu\n",
    "                best_c = c_temp  \n",
    "        except:\n",
    "            print(\"FAIL, lr:\", lear, \" l2:\", regu)   \n",
    "        i+=1\n",
    "        print(round(i/total_it,2), \"  \", end='')\n",
    "print(\"Parameter tuning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "disturbed-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG, M1: lr: 0.01 l2_reg: 0.01\n"
     ]
    }
   ],
   "source": [
    "lr_s5 = best_lear\n",
    "l2_reg_s5 = best_regu\n",
    "\n",
    "print(\"SHORT, M5: lr: {} l2_reg: {}\".format(lr_s5,l2_reg_s5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-tournament",
   "metadata": {},
   "source": [
    "Long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "completed-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST!! 0\n",
      "0.11   NEW BEST!! 1\n",
      "0.22   NEW BEST!! 2\n",
      "0.33   0.44   0.56   0.67   0.78   0.89   1.0   Parameter tuning done.\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "### Find most optimal parameters for every station\n",
    "best_c = -1\n",
    "best_lear = None\n",
    "best_regu = None\n",
    "best_model = None\n",
    "fails = {}\n",
    "\n",
    "total_it = len(learning_rates)*len(regularization)\n",
    "i=0\n",
    "\n",
    "# loop over parameters\n",
    "for lear in learning_rates:\n",
    "    for regu in regularization:\n",
    "        try: \n",
    "            cph = CoxPHModel()\n",
    "            cph.fit(X1_L, T_L, E_L,\n",
    "                    lr=lear, l2_reg = regu, init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "            #point pred\n",
    "            point_predictions = point_pred_single(cph,X1_L)\n",
    "\n",
    "            # c-index\n",
    "            c_temp = concordance_index(T_L, point_predictions)\n",
    "\n",
    "            if c_temp >= best_c:\n",
    "                print(\"NEW BEST!!\", i)\n",
    "                best_lear = lear\n",
    "                best_regu = regu\n",
    "                best_c = c_temp  \n",
    "        except:\n",
    "            print(\"FAIL, lr:\", lear, \" l2:\", regu)   \n",
    "        i+=1\n",
    "        print(round(i/total_it,2), \"  \", end='')\n",
    "print(\"Parameter tuning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "demonstrated-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG, M1: lr: 0.001 l2_reg: 1.0\n"
     ]
    }
   ],
   "source": [
    "lr_l1 = best_lear\n",
    "l2_reg_l1 = best_regu\n",
    "\n",
    "print(\"LONG, M1: lr: {} l2_reg: {}\".format(best_lear,best_regu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amber-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST!! 0\n",
      "0.11   0.22   NEW BEST!! 2\n",
      "0.33   NEW BEST!! 3\n",
      "0.44   NEW BEST!! 4\n",
      "0.56   0.67   0.78   NEW BEST!! 7\n",
      "0.89   1.0   Parameter tuning done.\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "### Find most optimal parameters for every station\n",
    "best_c = -1\n",
    "best_lear = None\n",
    "best_regu = None\n",
    "best_model = None\n",
    "fails = {}\n",
    "\n",
    "total_it = len(learning_rates)*len(regularization)\n",
    "i=0\n",
    "\n",
    "# loop over parameters\n",
    "for lear in learning_rates:\n",
    "    for regu in regularization:\n",
    "        try: \n",
    "            cph = CoxPHModel()\n",
    "            cph.fit(X5_L, T_L, E_L,\n",
    "                    lr=lear, l2_reg = regu, init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "            #point pred\n",
    "            point_predictions = point_pred_single(cph,X5_L)\n",
    "\n",
    "            # c-index\n",
    "            c_temp = concordance_index(T_L, point_predictions)\n",
    "\n",
    "            if c_temp >= best_c:\n",
    "                print(\"NEW BEST!!\", i)\n",
    "                best_lear = lear\n",
    "                best_regu = regu\n",
    "                best_c = c_temp  \n",
    "        except:\n",
    "            print(\"FAIL, lr:\", lear, \" l2:\", regu)   \n",
    "        i+=1\n",
    "        print(round(i/total_it,2), \"  \", end='')\n",
    "print(\"Parameter tuning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "higher-insured",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG, M1: lr: 0.1 l2_reg: 0.1\n"
     ]
    }
   ],
   "source": [
    "lr_l5 = best_lear\n",
    "l2_reg_l5 = best_regu\n",
    "\n",
    "print(\"LONG, M5: lr: {} l2_reg: {}\".format(best_lear,best_regu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-battery",
   "metadata": {},
   "source": [
    "Specify the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "indonesian-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to save results in \n",
    "results = {}\n",
    "\n",
    "# M1\n",
    "lr_s1 = 0.01\n",
    "l2_reg_s1 = 0.1\n",
    "lr_l1 = 0.001\n",
    "l2_reg_l1 = 1.0\n",
    "# M5\n",
    "lr_s5 = 0.01\n",
    "l2_reg_s5 = 0.01\n",
    "lr_l5 = 0.1\n",
    "l2_reg_l5 = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-remains",
   "metadata": {},
   "source": [
    "#### CPH - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "authentic-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.533285928333799\n",
      "0.5872982852279234\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#################### SEPERATE RESULTS ###################\n",
    "\n",
    "results['M1'] = {}\n",
    "results['M1']['S'] = {}\n",
    "results['M1']['L'] = {}\n",
    "results['M1']['Com'] = {}\n",
    "\n",
    "################ Fit models ################\n",
    "#\n",
    "####################### short #######################\n",
    "regs = CoxPHModel()\n",
    "regs.fit(X1_S, T_S, E_S, lr=lr_s1, l2_reg = l2_reg_s1, \n",
    "        init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "## Point predictions\n",
    "T_test_predS, T_train_predS = point_pred(regs, X1_S_test, X1_S)\n",
    "\n",
    "## Test model\n",
    "c_test = concordance_index(T_S_test, T_test_predS)\n",
    "c_train = concordance_index(T_S, T_train_predS)\n",
    "print(c_test)\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_S_test,T_test_predS)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_S,T_train_predS) \n",
    "\n",
    "## Save results\n",
    "results['M1']['S'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_S_test,T_test_predS), \n",
    "                      'R2_train':r2_score(T_S,T_train_predS),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}\n",
    "\n",
    "\n",
    "####################### long #######################\n",
    "regl = CoxPHModel()\n",
    "regl.fit(X1_L, T_L, E_L, lr=lr_l1, l2_reg = l2_reg_l1, \n",
    "        init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "## Point predictions\n",
    "T_test_predL, T_train_predL = point_pred(regl, X1_L_test, X1_L)\n",
    "\n",
    "## Test model\n",
    "c_test = concordance_index(T_L_test, T_test_predL)\n",
    "c_test = concordance_index(T_L, T_train_predL)\n",
    "print(c_test)\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_L_test,T_test_predL)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_L,T_train_predL) \n",
    "\n",
    "## Save results\n",
    "results['M1']['L'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_L_test,T_test_predL), \n",
    "                      'R2_train':r2_score(T_L,T_train_predL),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}\n",
    "\n",
    "################## Combined ##################\n",
    "T_com_real_test1 = pd.concat([T_S_test, T_L_test])\n",
    "T_test_predS.extend(T_test_predL)\n",
    "T_com_pred_test1 = T_test_predS\n",
    "\n",
    "T_com_real_train = pd.concat([T_S, T_L])\n",
    "T_train_predS.extend(T_train_predL)\n",
    "T_com_pred_train = T_train_predS\n",
    "\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_com_real_test1, T_com_pred_test1)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_com_real_train, T_com_pred_train) \n",
    "\n",
    "## C-index\n",
    "c_test = concordance_index(T_com_real_test1, T_com_pred_test1)\n",
    "c_train = concordance_index(T_com_real_train, T_com_pred_train)\n",
    "\n",
    "## Save results\n",
    "results['M1']['Com'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_com_real_test1,T_com_pred_test1), \n",
    "                      'R2_train':r2_score(T_com_real_train,T_com_pred_train),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-genome",
   "metadata": {},
   "source": [
    "#### CPH - Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "advance-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5341189018970465\n",
      "0.6001611205675602\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#################### SEPERATE RESULTS ###################\n",
    "\n",
    "results['M5'] = {}\n",
    "results['M5']['S'] = {}\n",
    "results['M5']['L'] = {}\n",
    "results['M5']['Com'] = {}\n",
    "\n",
    "################ Fit models ################\n",
    "#\n",
    "####################### short #######################\n",
    "regs = CoxPHModel()\n",
    "regs.fit(X5_S, T_S, E_S, lr=lr_s5, l2_reg = l2_reg_s5, \n",
    "        init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "## Point predictions\n",
    "T_test_predS, T_train_predS = point_pred(regs, X5_S_test, X5_S)\n",
    "\n",
    "## Test model\n",
    "c_test = concordance_index(T_S_test, T_test_predS)\n",
    "c_train = concordance_index(T_S, T_train_predS)\n",
    "print(c_test)\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_S_test,T_test_predS)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_S,T_train_predS) \n",
    "\n",
    "## Save results\n",
    "results['M5']['S'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_S_test,T_test_predS), \n",
    "                      'R2_train':r2_score(T_S,T_train_predS),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}\n",
    "\n",
    "####################### long #######################\n",
    "regl = CoxPHModel()\n",
    "regl.fit(X5_L, T_L, E_L, lr=lr_l5, l2_reg = l2_reg_l5, \n",
    "        init_method='zeros', max_iter=800, verbose=False)\n",
    "\n",
    "## Point predictions\n",
    "T_test_predL, T_train_predL = point_pred(regl, X5_L_test, X5_L)\n",
    "\n",
    "## Test model\n",
    "c_test = concordance_index(T_L_test, T_test_predL)\n",
    "c_train = concordance_index(T_L, T_train_predL)\n",
    "print(c_test)\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_L_test,T_test_predL)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_L,T_train_predL) \n",
    "\n",
    "## Save results\n",
    "results['M5']['L'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_L_test,T_test_predL), \n",
    "                      'R2_train':r2_score(T_L,T_train_predL),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}\n",
    "\n",
    "## Point predictions\n",
    "T_test_predL, T_train_predL = point_pred(regl, X5_L_test, X5_L)\n",
    "\n",
    "################## Combined ##################\n",
    "T_com_real_test5 = pd.concat([T_S_test, T_L_test])\n",
    "T_test_predS.extend(T_test_predL)\n",
    "T_com_pred_test5 = T_test_predS\n",
    "\n",
    "T_com_real_train = pd.concat([T_S, T_L])\n",
    "T_train_predS.extend(T_train_predL)\n",
    "T_com_pred_train = T_train_predS\n",
    "\n",
    "## Get metrics\n",
    "MAE_test, RMSE_test, MSE_test, MAPE_test, NRMSE_test = test_model(T_com_real_test5, T_com_pred_test5)\n",
    "MAE_train, RMSE_train, MSE_train, MAPE_train, NRMSE_train = test_model(T_com_real_train, T_com_pred_train) \n",
    "\n",
    "## C-index\n",
    "c_test = concordance_index(T_com_real_test5, T_com_pred_test5)\n",
    "c_train = concordance_index(T_com_real_train, T_com_pred_train)\n",
    "\n",
    "## Save results\n",
    "results['M5']['Com'] = {'RMSE_train':RMSE_train,'RMSE_test':RMSE_test,\n",
    "                      'MAE_train': MAE_train,'MAE_test':MAE_test,\n",
    "                      'R2_test':r2_score(T_com_real_test1,T_com_pred_test1), \n",
    "                      'R2_train':r2_score(T_com_real_train,T_com_pred_train),\n",
    "                      'MAPE_test':MAPE_test,'MAPE_train':MAPE_train,\n",
    "                      'NRMSE_test':NRMSE_test,'NRMSE_train':NRMSE_train,\n",
    "                      'c-train':c_train,'c-test':c_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-kenya",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "neither-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORT\n",
      "DeepSurv M1: R2: 0.01 RMSE: 396.0 C-idx: 0.53 MAE: 309.0 MAPE: 2008.0 NRMSE: 1.4\n",
      "DeepSurv M5: R2: 0.01 RMSE: 397.0 C-idx: 0.53 MAE: 306.0 MAPE: 1957.0 NRMSE: 1.41\n"
     ]
    }
   ],
   "source": [
    "models = ['M1', 'M5']\n",
    "\n",
    "print(\"SHORT\")\n",
    "for mo in models:\n",
    "    r = results[mo]['S']   \n",
    "    RMSE = r['RMSE_test']\n",
    "    R2 = r['R2_test']\n",
    "    MAE = r['MAE_test']\n",
    "    MAPE = r['MAPE_test']\n",
    "    NRMSE = r['NRMSE_test']\n",
    "    C_idx = r['c-test']\n",
    "\n",
    "    print(\"DeepSurv {}: R2: {} RMSE: {} C-idx: {} MAE: {} MAPE: {} NRMSE: {}\".format(mo,\n",
    "                                                                          round(R2,2),\n",
    "                                                                          round(RMSE,0),\n",
    "                                                                          round(C_idx,2),\n",
    "                                                                          round(MAE,0),\n",
    "                                                                          round(MAPE,0),\n",
    "                                                                          round(NRMSE,2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "amazing-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG\n",
      "DeepSurv M1: R2: 0.04 RMSE: 443.0 C-idx: 0.59 MAE: 326.0 MAPE: 1417.0 NRMSE: 0.59\n",
      "DeepSurv M5: R2: -0.01 RMSE: 454.0 C-idx: 0.6 MAE: 342.0 MAPE: 1312.0 NRMSE: 0.61\n"
     ]
    }
   ],
   "source": [
    "models = ['M1', 'M5']\n",
    "\n",
    "print(\"LONG\")\n",
    "for mo in models:\n",
    "    r = results[mo]['L']   \n",
    "    RMSE = r['RMSE_test']\n",
    "    R2 = r['R2_test']\n",
    "    MAE = r['MAE_test']\n",
    "    MAPE = r['MAPE_test']\n",
    "    NRMSE = r['NRMSE_test']\n",
    "    C_idx = r['c-test']\n",
    "\n",
    "    print(\"DeepSurv {}: R2: {} RMSE: {} C-idx: {} MAE: {} MAPE: {} NRMSE: {}\".format(mo,\n",
    "                                                                          round(R2,2),\n",
    "                                                                          round(RMSE,0),\n",
    "                                                                          round(C_idx,2),\n",
    "                                                                          round(MAE,0),\n",
    "                                                                          round(MAPE,0),\n",
    "                                                                          round(NRMSE,2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "stable-moment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined\n",
      "DeepSurv M1: R2: 0.25 RMSE: 415.0 C-idx: 0.65 MAE: 316.0 MAPE: 1781.0 NRMSE: 0.9\n",
      "DeepSurv M5: R2: 0.25 RMSE: 420.0 C-idx: 0.65 MAE: 320.0 MAPE: 1709.0 NRMSE: 0.91\n"
     ]
    }
   ],
   "source": [
    "models = ['M1', 'M5']\n",
    "\n",
    "print(\"Combined\")\n",
    "for mo in models:\n",
    "    r = results[mo]['Com']   \n",
    "    RMSE = r['RMSE_test']\n",
    "    R2 = r['R2_test']\n",
    "    MAE = r['MAE_test']\n",
    "    MAPE = r['MAPE_test']\n",
    "    NRMSE = r['NRMSE_test']\n",
    "    C_idx = r['c-test']\n",
    "\n",
    "    print(\"DeepSurv {}: R2: {} RMSE: {} C-idx: {} MAE: {} MAPE: {} NRMSE: {}\".format(mo,\n",
    "                                                                          round(R2,2),\n",
    "                                                                          round(RMSE,0),\n",
    "                                                                          round(C_idx,2),\n",
    "                                                                          round(MAE,0),\n",
    "                                                                          round(MAPE,0),\n",
    "                                                                          round(NRMSE,2))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
